{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.37.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r scripts/requirements.txt (line 1)) (4.37.0)\n",
      "Requirement already satisfied: peft==0.8.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r scripts/requirements.txt (line 2)) (0.8.2)\n",
      "Requirement already satisfied: datasets==2.17.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (2.17.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.99 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r scripts/requirements.txt (line 4)) (0.1.99)\n",
      "Requirement already satisfied: bitsandbytes==0.42.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r scripts/requirements.txt (line 5)) (0.42.0)\n",
      "Requirement already satisfied: mlflow==2.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r scripts/requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r scripts/requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r scripts/requirements.txt (line 1)) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from peft==0.8.2->-r scripts/requirements.txt (line 2)) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from peft==0.8.2->-r scripts/requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from peft==0.8.2->-r scripts/requirements.txt (line 2)) (0.31.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (3.9.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r scripts/requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (0.18.0)\n",
      "Requirement already satisfied: entrypoints<1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=2.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.1.43)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (4.25.3)\n",
      "Requirement already satisfied: pytz<2024 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (2023.4)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (6.11.0)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (0.5.0)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.13.2)\n",
      "Requirement already satisfied: docker<7,>=4.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (6.1.3)\n",
      "Requirement already satisfied: Flask<3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (2.3.3)\n",
      "Requirement already satisfied: querystring-parser<2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.2.4)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (2.0.29)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.4.1.post1)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.6)\n",
      "Requirement already satisfied: matplotlib<4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.8.3)\n",
      "Requirement already satisfied: gunicorn<22 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (21.2.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: Mako in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (4.10.0)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (2.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.2.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from docker<7,>=4.0.0->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.7.0)\n",
      "Requirement already satisfied: Werkzeug>=2.3.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from Flask<3->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from Flask<3->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from Flask<3->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (2.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers==4.37.0->-r scripts/requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers==4.37.0->-r scripts/requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers==4.37.0->-r scripts/requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (12.5.40)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from s3fs->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (1.34.101)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs->datasets[s3]==2.17.1->-r scripts/requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.6.0->-r scripts/requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.8.2->-r scripts/requirements.txt (line 2)) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::211125449279:role/service-role/AmazonSageMaker-ExecutionRole-20240307T175168\n",
      "sagemaker bucket: sagemaker-eu-west-1-211125449279\n",
      "sagemaker session region: eu-west-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and store the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9385a2018249fba14a18a7a361209e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae5f5377e4c41e18591b0bd9f8b2dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d25413e477c4f278b15bd60423e4912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbddc05c59cd4db0b77c5902e2a8fae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the dataset and the model to be downloaded\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Union\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../env.txt\")\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# dataset used\n",
    "data_path = 'MichaelAI23/English_CVs'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/English_CVs'\n",
    "\n",
    "cutoff_len = 1024 # 512\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "use_special_token = True\n",
    "\n",
    "if use_special_token:\n",
    "    # Add the single quote as special token for faster inference (only fill the blanks --> no embedding training necessary, since token exists already)\n",
    "    tokenizer.add_tokens([\"\\'\"], special_tokens=True) #, \"}, \"])\n",
    "\n",
    "    assert len(tokenizer) == 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = {\n",
    "    \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\nInstruction:\\n{instruction}\\n\\nInput:\\n{input}\\n\\nResponse:\\n\",\n",
    "    \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\nInstruction:\\n{instruction}\\n\\nResponse:\\n\",\n",
    "    \"response_split\": \"Response:\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(\n",
    "    template: str,\n",
    "    instruction: str,\n",
    "    input: Union[None, str] = None,\n",
    "    label: Union[None, str] = None,\n",
    ") -> str:\n",
    "    # returns the full prompt from instruction and optional input\n",
    "    # if a label (=response, =output) is provided, it's also appended.\n",
    "    if label:\n",
    "        res = template[\"prompt_input\"].format(\n",
    "            instruction=instruction, input=input\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": res},\n",
    "            {\"role\": \"assistant\", \"content\": f\" {label}\"}\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        res = template[\"prompt_input\"].format(\n",
    "            instruction=instruction, input=input\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": res},\n",
    "        ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def tokenize(messages):\n",
    "    # Create the final prompt (with the template and tokenize it)\n",
    "    final_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    result = tokenizer(\n",
    "        final_prompt,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False, # this is already done by the application of the chat template\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    result[\"full_prompt\"] = final_prompt\n",
    "\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(\n",
    "        template,\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820974769e6a4d778c258961b2b868cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/538 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a963547cec84833bdf66ddc30b2952c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/877k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7085b4d1eb4e7d80be93162ce4353a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'hobbies', 'personal', 'work_experience', 'skills', 'academia', 'education', 'overall'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(data_path)\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levi Cardoso can be reached at levi.cardoso@wynnlasvegas.com for any inquiries. His address is 1234 Wynn Way, Las Vegas, NV, USA. You can contact him at (555) 123-4567. Levi Cardoso is a dedicated employee at The Wynn in Las Vegas and always ready to assist with any requests.\n",
      "\n",
      "Proficient in Python, R, and SQL for data manipulation and analysis. Skilled in machine learning algorithms and statistical modeling to derive actionable insights from complex datasets.\n",
      "\n",
      "I enjoy skiing during the winter months, exploring new hiking trails in the summer, and practicing yoga for relaxation. Additionally, I have a passion for cooking and experimenting with new recipes in my spare time.\n",
      "\n",
      "A bachelor's degree in Computer Science from Universidad Complutense de Madrid in 2010 and a master's degree in Data Science from Universidad Politécnica de Madrid in 2014 were successfully completed.\n",
      "\n",
      "Data Scientist, 2014 - 2016 at Google\n",
      "- Developed machine learning models to optimize search engine algorithms and improve user experience.\n",
      "- Collaborated with cross-functional teams to extract insights from large datasets and drive data-driven decision-making processes.\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][\"overall\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pe': {'s': 'Levi', 'e': ' requests.'},\n",
       " 'ed': {'s': 'A bachelor', 'e': ' completed.'},\n",
       " 'wo': {'s': 'Data Scientist', 'e': ' processes.'},\n",
       " 'sk': {'s': 'Proficient', 'e': ' datasets.'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "output = data[\"train\"][\"output\"][10]\n",
    "output = re.sub(\"\\'\", \"\\\"\", output)\n",
    "output = output.replace(\"\\\\n\", \"\")\n",
    "json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Benjamin Costa is a guest at The Taj Exotica Resort & Spa in the Maldives. He can be reached via email at bencosta@example.com. His address is Villa 12, Oceanfront Drive, South Male Atoll, Maldives. For any inquiries or reservations, you can contact him at +960-123-4567.\\nI enjoy swimming as a hobby, as it allows me to relax and stay active outside of my work as a Data Scientist. In addition, I also love hiking in the mountains and playing the guitar in my free time.\\nData Scientist, 2014 - 2016 at Google\\n- Developed machine learning models to optimize search algorithms.\\n- Conducted data analysis and visualization to extract actionable insights from large datasets.\\n- Collaborated with cross-functional teams to implement data-driven solutions for product enhancements.\\n- Presented findings to stakeholders and provided recommendations for business strategies.\\n\\nSenior Data Scientist, 2012 - 2014 at Amazon\\n- Led a team of data scientists in developing predictive analytics models for customer behavior forecasting.\\n- Implemented scalable data pipelines using cloud-based technologies for real-time data processing.\\n- Worked closely with product managers to define key performance indicators and measure the impact of data-driven initiatives.\\n- Mentored junior data scientists and conducted training sessions on advanced analytical techniques.\\n- Master of Science in Data Science, Politecnico di Milano, 2012\\n- Bachelor of Science in Computer Science, Università degli Studi di Milano, 2009\\nProficient in statistical analysis and data visualization techniques, with a strong background in machine learning algorithms. Skilled at deriving actionable insights from complex datasets to drive informed decision-making and business growth.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"] = data[\"train\"].add_column(\n",
    "    \"instruction\",\n",
    "    [\"Extract the start and end sequences for the categories 'personal information', 'work experience', 'education' and 'skills' from the following text in dictionary form\"]*len(data[\"train\"])\n",
    ")\n",
    "\n",
    "data[\"train\"] = data[\"train\"].rename_column(\"overall\", \"input\")\n",
    "data[\"train\"][\"input\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_set_size= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if val_set_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=True, seed=42\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'hobbies', 'personal', 'work_experience', 'skills', 'academia', 'education', 'input', 'instruction', 'input_ids', 'attention_mask', 'full_prompt', 'labels'],\n",
       "    num_rows: 900\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- I enjoy playing golf in my free time and often participate in local tournaments.\\n- Photography is another passion of mine, capturing beautiful moments and landscapes brings me joy.\\n- Cooking is a relaxing hobby for me, experimenting with new recipes and flavors is always exciting.\\n- I love hiking and exploring nature trails on the weekends, it helps me unwind and stay active.\\n- Reading fiction novels is a favorite pastime of mine, getting lost in a good book is a great escape from reality.\\n\\nAs a seasoned Business Analyst, I excel in translating complex business requirements into actionable insights that drive strategic decision-making. With a keen eye for detail and a knack for data analysis, I have a proven track record of delivering innovative solutions to enhance operational efficiency and maximize profitability. My strong communication skills enable me to effectively collaborate with cross-functional teams and stakeholders to achieve project objectives seamlessly.\\n\\n- Bachelor of Business Administration, University of Melbourne, 2017\\n- Master of Business Analytics, Monash University, 2020\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[\"input\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 16:31:42.466838: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 16:31:42.466996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 16:31:42.468254: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 16:31:42.478998: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 16:31:43.879026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\nInstruction:\\nExtract the start and end sequences for the categories ' personal information' , ' work experience' , ' education'  and ' skills'  from the following text in dictionary form\\n\\nInput:\\n\\nProficient in statistical analysis and data visualization techniques, with a strong background in machine learning algorithms. Skilled at deriving actionable insights from complex datasets to drive informed decision-making and business growth.\\n- I enjoy dancing salsa and bachata in my free time.\\n- I am an avid hiker and love exploring new trails in the mountains.\\n- Painting is a relaxing hobby of mine, and I often create abstract art pieces.\\n- Cooking is another passion of mine, and I love experimenting with different cuisines.\\n- I have a green thumb and enjoy gardening, especially growing herbs and vegetables.\\n- Reading fiction novels is a favorite pastime of mine, and I always have a book on hand.\\nData Scientist, 2019 - Present at Amazon\\n- Conducted advanced data analysis to optimize customer segmentation strategies.\\n- Developed machine learning models to predict sales trends and improve inventory management.\\n\\nJunior Data Scientist, 2017 - 2019 at Google\\n- Collaborated with cross-functional teams to design and implement data-driven solutions for ad targeting.\\n- Utilized natural language processing techniques to enhance search engine algorithms and improve user experience.\\n- Mail: harper.andrade@ritzcarlton.com\\n- Address: The Ritz-Carlton, Amelia Island, 4750 Amelia Island Pkwy, Fernandina Beach, FL 32034, USA\\n- Phone: +1 (904) 277-1100\\n- Name: Harper Andrade\\n- LinkedIn: linkedin.com/in/harper-andrade\\n\\nResponse:\\n [/INST] {' pe' : {' s' : ' - Mail' , ' e' : ' -andrade' }, ' ed' : {' s' : '' , ' e' : '' }, ' wo' : {' s' : ' Data' , ' e' : '  experience.' }, ' sk' : {' s' : ' Proficient' , ' e' : '  growth.' }}</s>\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_data[\"input_ids\"][2]) #[:len(train_data[\"input_ids\"][2]) - 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28742"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"input_ids\"][2][:len(train_data[\"input_ids\"][2]) - 63][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data.map(lambda example: {\"token_len\": len(example[\"input_ids\"])})\n",
    "val_data = val_data.map(lambda example: {\"token_len\": len(example[\"input_ids\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_data[\"token_len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if max(train_data[\"token_len\"]) > cutoff_len or max(val_data[\"token_len\"]) > cutoff_len:\n",
    "    raise ValueError(\"You have samples that are longer than your cutoff length. This can lead to unintended side consequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_data.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "val_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/val'\n",
    "val_data.save_to_disk(val_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning & starting Sagemaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "time_string = time.strftime(\"%H-%M-%S\")\n",
    "\n",
    "base_job_name=f\"LLM-Textmarker-{base_model}-{time_string}\"\n",
    "base_job_name = re.sub(r\"[_/\\.]\", \"-\", base_job_name)\n",
    "checkpoint_in_bucket=\"checkpoints\"\n",
    "\n",
    "# The S3 URI to store the checkpoints\n",
    "checkpoint_s3_bucket=\"s3://{}/{}/{}\".format(sess.default_bucket(), base_job_name, checkpoint_in_bucket)\n",
    "\n",
    "# The local path where the model will save its checkpoints in the training container\n",
    "checkpoint_local_path=\"/opt/ml/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'epochs': 5,\n",
    "    # 'batch_size': 8,\n",
    "    'batch_size': 1,\n",
    "    'base_model': base_model,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'num_epochs': 5,\n",
    "    'learning_rate': 3e-4,\n",
    "    'cutoff_len': cutoff_len,\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0.05,\n",
    "    'group_by_length': False,\n",
    "    'device_map': 'auto',\n",
    "    'model_dir': checkpoint_local_path,\n",
    "    'use_special_token': use_special_token\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train_mistral.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.g5.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            base_job_name=base_job_name,\n",
    "                            role=role,\n",
    "                            transformers_version='4.36',\n",
    "                            pytorch_version='2.1',\n",
    "                            py_version='py310',\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            # Parameters required to enable checkpointing\n",
    "                            checkpoint_s3_uri=checkpoint_s3_bucket,\n",
    "                            checkpoint_local_path=checkpoint_local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'val': val_input_path}, job_name=base_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tar the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-west-1-211125449279/LLM-Textmarker-mistralai-Mistral-7B-Instruct-v0-2-15-24-12/checkpoints/model_files/adapter_config.json to model/adapter_config.json\n",
      "download: s3://sagemaker-eu-west-1-211125449279/LLM-Textmarker-mistralai-Mistral-7B-Instruct-v0-2-15-24-12/checkpoints/model_files/adapter_model.safetensors to model/adapter_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://sagemaker-eu-west-1-211125449279/LLM-Textmarker-mistralai-Mistral-7B-Instruct-v0-2-15-24-12/checkpoints/model_files ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar zcvf model.tar.gz ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp model.tar.gz s3://sagemaker-eu-west-1-211125449279/LLM-Textmarker-mistralai-Mistral-7B-Instruct-v0-2-15-24-12/checkpoints/model_files/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Deploying the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = huggingface_estimator.deploy(1, \"ml.g4dn.2xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_model()\n",
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
