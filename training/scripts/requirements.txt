transformers==4.37.0
peft==0.8.2
datasets[s3]==2.17.1
sentencepiece==0.1.99
bitsandbytes==0.42.0
# mlflow==2.6.0
python-dotenv

# flash-attention
# git clone -b v2.4.2 https://github.com/Dao-AILab/flash-attention
# cd flash-attention && pip install .