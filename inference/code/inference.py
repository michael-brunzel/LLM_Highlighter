from sagemaker_inference.decoder import decode
from sagemaker_inference.encoder import encode
from typing import List
from collections import OrderedDict
import json
import logging
import torch
from typing import List
from typing import Dict, Any
from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria
import torch

from peft import AutoPeftModelForCausalLM
from huggingface_hub import login
import os
login(token=os.environ.get("HF_TOKEN"))


simple_json_schema = OrderedDict({
        'pe': {
            's': {"type": "string"},
            'e': {"type": "string"},
        },
        'ed': {
            's': {"type": "string"},
            'e': {"type": "string"},
        },
        'wo': {
            's': {"type": "string"},
            'e': {"type": "string"},
        },
        'sk': {
            's': {"type": "string"},
            'e': {"type": "string"},
        }
})


class MyStoppingCriteria(StoppingCriteria):
    def __init__(self, target_sequence, prompt, tokenizer):
        self.target_sequence = target_sequence
        self.prompt = prompt
        self.tokenizer = tokenizer

    def __call__(self, input_ids, scores, **kwargs):
        # Get the generated text as a string
        generated_text = self.tokenizer.decode(input_ids[0])
        generated_text = generated_text.replace(self.prompt, '')
        # Check if the target sequence appears in the generated text
        if self.target_sequence in generated_text:
            return True  # Stop generation

        return False  # Continue generation

    def __len__(self):
        return 1

    def __iter__(self):
        yield self


class EndpointHandler:
    def __init__(self, model_dir=""):
        # load model and processor from path
        self.tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")   #("MichaelAI23/mistral_7B_v0_2_Textmarker_special_token") #, subfolder="model") #mistralai/Mistral-7B-Instruct-v0.2")
        self.tokenizer.add_tokens("\'", special_tokens=True)
        # attn_implementation="flash_attention_2"
        self.model = AutoPeftModelForCausalLM.from_pretrained(
            "MichaelAI23/mistral_7B_v0_2_Textmarker_special_token", # use the current folder
            # subfolder="model",
            device_map="auto",
            torch_dtype=torch.bfloat16
            # attn_implementation="flash_attention_2"
        ) # load_in_4bit=True

        self.template = {
            "prompt_input": """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\nInstruction:\n{instruction}\n\nInput:\n{input}\n\nResponse:\n"""
        }
        self.instruction = """Extract the start and end sequences for the categories 'personal information', 'work experience', 'education' and 'skills' from the following text in dictionary form"""

        if torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"


    def __call__(self, data: Dict[str, Any]) -> Dict[str, str]:
        """
        Args:
            data (dict): The payload with the text prompt and generation parameters.
        """
        # process input
        inputs = data.pop("inputs", data)
        parameters = data.pop("parameters", None)

        res = self.template["prompt_input"].format(
            instruction=self.instruction, input=inputs
        )

        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        jsonformer = ModifiedJsonFormer()
        json_string_splitted = jsonformer.create_schema_splits(simple_json_schema)
        prediction = jsonformer.modified_jsonformer(self.model, self.tokenizer, self.device, res, json_string_splitted, parameters)

        return [{"generated_text": prediction}]


class ModifiedJsonFormer():
    def create_schema_splits(self, json_schema: dict) -> List[str]:
        """
        Prepare the different string snippets such that only the fields get
        generated by the LLM.

        Args:
            json_schema(dict): The desired schema of the LLM output
        """
        json_string = json.dumps(json_schema).replace("\"", "\'")
        json_string_splitted = json_string.split("""{'type': 'string'}""")
        json_string_splitted[0] = f" {json_string_splitted[0]}"
        return json_string_splitted

    def modified_jsonformer(
            self, model, tokenizer, device, res, json_string_splitted, parameters
        ):
        """
        Code for adding the incremental snippets..
        """        
        llm_string = ""
        for entry in json_string_splitted[:-1]:
            llm_string += entry + "\'"

            messages = [
                {"role": "user", "content": res},
                {"role": "assistant", "content": llm_string}
            ]

            input_ids = tokenizer.apply_chat_template(
                messages, truncation=True, add_generation_prompt=True, return_tensors="pt"
            )
            current_prompt = tokenizer.decode(input_ids[0][:-1]) # the end-token needs to be removed
            input_ids = input_ids.to(device)
            # print(f"current prompt: {current_prompt}")

            outputs = model.generate(
                input_ids=input_ids[:, :-1], #[:, :len(input_ids[0])-1],
                stopping_criteria=MyStoppingCriteria("\'", current_prompt, tokenizer),
                **parameters)
            # print(f"decoded output: {tokenizer.decode(outputs[0])}")
            decoded_string = tokenizer.decode(outputs[0][(input_ids.shape[1]-1):]) #handler.tokenizer.decode(outputs[0])
            decoded_string = "".join(decoded_string.rsplit("\'")[0]) + "\'" # cut off everythin after the last quote
            # print(f"Decode string: {decoded_string}")
            llm_string += decoded_string #"placeholder\"" # this needs to be generated by the model
            # print(f"current llm string {llm_string}")
        final_string = llm_string + json_string_splitted[-1]
        return final_string

def model_fn(model_dir, context=None):
    # implement custom code to load the model
    loaded_model = EndpointHandler(model_dir=model_dir)
    
    return loaded_model 

def transform_fn(model, input_data, content_type, accept, context=None):
     # decode the input data (e.g. JSON string -> dict)
    data = decode(input_data, content_type).tolist()

    # call your custom model with the data
    outputs = model(data) # , ... ) 

    # convert the model output to the desired output format (e.g. dict -> JSON string)
    response = encode(outputs, accept)

    return response