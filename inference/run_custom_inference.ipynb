{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d432f4-5524-4043-bce1-022a423469be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.37.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r code/requirements.txt (line 1)) (4.37.0)\n",
      "Requirement already satisfied: tokenizers==0.15.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r code/requirements.txt (line 2)) (0.15.2)\n",
      "Requirement already satisfied: peft==0.8.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r code/requirements.txt (line 3)) (0.8.2)\n",
      "Requirement already satisfied: datasets==2.17.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (2.17.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.99 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r code/requirements.txt (line 5)) (0.1.99)\n",
      "Requirement already satisfied: bitsandbytes==0.42.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r code/requirements.txt (line 6)) (0.42.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r code/requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r code/requirements.txt (line 1)) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r code/requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r code/requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r code/requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r code/requirements.txt (line 1)) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r code/requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r code/requirements.txt (line 1)) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers==4.37.0->-r code/requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from peft==0.8.2->-r code/requirements.txt (line 3)) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from peft==0.8.2->-r code/requirements.txt (line 3)) (2.4.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from peft==0.8.2->-r code/requirements.txt (line 3)) (0.33.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (3.9.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r code/requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (0.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0->-r code/requirements.txt (line 1)) (4.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.37.0->-r code/requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers==4.37.0->-r code/requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers==4.37.0->-r code/requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers==4.37.0->-r code/requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers==4.37.0->-r code/requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (12.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from s3fs->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (1.34.142)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.17.1->datasets[s3]==2.17.1->-r code/requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.8.2->-r code/requirements.txt (line 3)) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cdf720-dbe2-4c65-ac2f-6b9da96fa4da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73e7e915-a49c-4a41-b254-ab84eec0c42c",
   "metadata": {},
   "source": [
    "#### Before we can create our custom sagemaker endpoint we need to tar all the necessary files in an s3-location (For details see: https://huggingface.co/docs/sagemaker/inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7751c9f5-0c22-4146-9b33-9a546b078489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ec2-user/SageMaker/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "904e18f4-ad34-4153-97b1-e011eb1822f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./adapter_model.safetensors\n",
      "./code/\n",
      "./code/inference.py\n",
      "./code/.ipynb_checkpoints/\n",
      "./code/.ipynb_checkpoints/inference-checkpoint.py\n",
      "./code/.ipynb_checkpoints/requirements-checkpoint.txt\n",
      "./code/requirements.txt\n",
      "./adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# aws s3 cp s3://sagemaker-eu-west-1-211125449279/LLM-Textmarker-mistralai-Mistral-7B-Instruct-v0-2-16-43-42/checkpoints/model_files ./tar_folder --recursive\n",
    "# cp ./LLM_Highlighter/inference/code ./tar_folder/code --recursive\n",
    "tar zcvf model.tar.gz -C ./tar_folder/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62e34b6b-c2b5-472f-82bb-b61a6e382770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-eu-west-1-211125449279/LLM-Textmarker-mistralai-Mistral-7B-Instruct-v0-2-16-43-42/checkpoints/custom_inference/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp model.tar.gz s3://sagemaker-eu-west-1-211125449279/LLM-Textmarker-mistralai-Mistral-7B-Instruct-v0-2-16-43-42/checkpoints/custom_inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2efc5eb1-d168-45d8-9149-f4c977fc1f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_location = \"s3://sagemaker-eu-west-1-211125449279/LLM-Textmarker-mistralai-Mistral-7B-Instruct-v0-2-16-43-42/checkpoints/custom_inference/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b499aa5-f8b2-4416-8cba-51ca3015ccc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::211125449279:role/service-role/AmazonSageMaker-ExecutionRole-20240307T175168\n",
      "sagemaker bucket: sagemaker-eu-west-1-211125449279\n",
      "sagemaker session region: eu-west-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d64ee5c8-53ed-46c3-a94c-47bb83a5d312",
   "metadata": {},
   "source": [
    "#### Create the actual Sagemaker endpoint (only reachable from within AWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5ec1469-1b0d-46eb-abe5-5b9c7cca3ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"LLM_Highlighter/env.txt\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "env_dict = {\n",
    "    \"HF_TOKEN\": hf_token\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_location,       # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.37\",  # transformers version used\n",
    "   pytorch_version=\"2.1\",        # pytorch version used\n",
    "   env=env_dict, # env variables\n",
    "   py_version='py310',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf80d86f-cffc-4621-ace9-5fb08c6be6e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset used\n",
    "data_path = 'MichaelAI23/English_CVs'\n",
    "\n",
    "data = load_dataset(data_path)\n",
    "\n",
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=100, shuffle=True, seed=42\n",
    ")\n",
    "\n",
    "train_data = train_val[\"train\"]\n",
    "val_data = train_val[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bf8748a-e2f0-422f-abd7-664179cd01ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As a Business Analyst at TechSolutions Inc., I was responsible for analyzing market trends and providing strategic recommendations to drive business growth. I started my role in January 2014 after graduating in 2012 and continued until December 2018, where I successfully implemented data-driven solutions that improved operational efficiency.\\n- Published research paper on \"The Impact of Data Analytics in Business Decision Making\" in Journal of Business Intelligence, 2012\\n- Co-authored a study on \"Predictive Modeling for Customer Churn Analysis\" in International Conference on Business Analytics, 2013\\n- Presented findings on \"Optimizing Supply Chain Management through Big Data Analysis\" at the Annual Meeting of Operations Research Society, 2014\\n- Contributed to a book chapter titled \"Emerging Trends in Business Forecasting Techniques\" published by Springer, 2015\\n- Received Best Paper Award for research on \"Machine Learning Applications in Marketing Strategy Development\" at the Academy of Management Conference, 2016\\nExperienced Business Analyst with a proven track record of leveraging data-driven insights to drive strategic decision-making and improve operational efficiency. Skilled in conducting thorough market research, analyzing complex business processes, and developing innovative solutions to optimize performance. Proficient in translating technical requirements into user-friendly reports and presentations for stakeholders at all levels. Adept at collaborating cross-functionally to deliver actionable recommendations that align with organizational goals and drive sustainable growth.\\nMichael Lima is a guest at The Burj Al Arab Jumeirah in Dubai, UAE. His mailing address is P.O. Box 74147, Dubai, United Arab Emirates. He can be reached by phone at +971-4-3017777. Michael Lima enjoys the luxurious amenities and stunning views offered by the iconic hotel.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0][\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3470613-ab03-4bb4-9300-dd5dc604fbc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Experienced Data Engineer with a strong foundation in designing and implementing scalable data pipelines for complex analytics projects. Proficient in utilizing cutting-edge technologies such as Apache Spark, Hadoop, and Kafka to optimize data processing and storage. Skilled in collaborating with cross-functional teams to deliver innovative solutions that drive business growth and enhance decision-making capabilities.\\n- I enjoy skiing in the winter and hitting the slopes whenever I get the chance.\\n- Photography is a passion of mine, capturing moments and scenes that inspire me.\\n- Cooking is a relaxing hobby for me, experimenting with new recipes and flavors in the kitchen.\\n- Traveling to new destinations and immersing myself in different cultures is something I love to do in my free time.\\n- Playing the guitar helps me unwind and express myself creatively outside of work.\\n- Mail: ajamirez@peninsula.com\\n- Address: 108 E Superior St, Chicago, IL 60611, USA\\n- Phone: +1 (312) 337-2888\\n- Name: Alessandro Jamirez\\n- LinkedIn: linkedin.com/in/alessandrojamirez\\nA bachelor's degree in Computer Science from a prestigious French university in 2011 and a master's degree in Data Engineering from the same institution in 2014 were successfully completed.\\nData Engineer, 2014 - 2016 at TechCorp\\n- Developed and maintained ETL pipelines to ingest data from various sources into a centralized data warehouse.\\n- Designed and implemented scalable data processing solutions using Hadoop and Spark technologies.\\n- Collaborated with cross-functional teams to optimize data storage and retrieval processes for improved performance.\\n- Conducted regular data quality checks and troubleshooting to ensure accuracy and reliability of datasets.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[32][\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58293555-466c-4256-9114-0861c9dba43a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.286539077758789 seconds ---\n",
      "[{'generated_text': \" {'pe': {'s': '- Mail', 'e': '/alessandrojamirez'}, 'ed': {'s': 'A bachelor', 'e': ' completed.'}, 'wo': {'s': 'Data Engineer, 2014', 'e': ' datasets.'}, 'sk': {'s': 'Experienced', 'e': 'capabilities.'}}\"}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "data = {\n",
    "    \"inputs\": val_data[32][\"overall\"],\n",
    "    \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0}\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e91fb453-79ce-4c6c-87fd-fb62d7f92949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
